= Lab setup and introduction

== Module goals

* Access all of the applications in the lab environment
* Deploy our default insecure applications
* Ensuring OpenShift pipelines are properly setup

== Accessing the workshop

Let's start by ensuring you have access to all the necessary resources to complete this lab. 

=== Access the Red Hat^(R)^ OpenShift Container Platform (OCP) web console 

First, make sure you can access the Red Hat^(R)^ OCP console web console.

*Procedure*

[start=1]
. Log into the OCP console at `{web_console_url}`
. Click the *rhsso* option

image::01-ocp-login-admin.png[OpenShift console]

[start=3]
. Enter the OCP credentials 

[cols="1,1"]
|===
*User:*| {openshift_admin_user} |
*Password:*| {openshift_admin_password} |
|===


[start=4]
. Enter the OpenShift username *{openshift_admin_user}* and password: *{openshift_admin_password}*

image::01-ocp-login-password.png[OpenShift console login]

IMPORTANT: If the variables on the screen are unavailable, please inform your workshop administrator.

=== Access the Red Hat^(R)^ Advanced Cluster Security (RHACS) web console 

Red Hat Advanced Cluster Security for Kubernetes is a Kubernetes-native security platform that equips you to build, deploy, and run cloud-native applications with more security. The solution helps protect containerized Kubernetes workloads in all major clouds and hybrid platforms, including Red Hat OpenShift, Amazon Elastic Kubernetes Service (EKS), Microsoft Azure Kubernetes Service (AKS), and Google Kubernetes Engine (GKE).

First, access the RHACS web console.

*Procedure*

[start=1]
. Log into the RHACS console at `{acs_route}`
. Click the "Advanced" button in your browser

image::01-rhacs-advanced.png[RHACS login not private] 

[start=3]
. Click "Proceed to {acs_route}"

image::01-rhacs-proceed.png[RHACS login proceed]

[start=4]
. Enter the RHACS credentials 

[cols="1,1"]
|===
*RHACS Console Username:* | {acs_portal_username} |
*RHACS Console Password:* | {acs_portal_password} |
|===

image::01-rhacs-login.png[RHACS console]

image::01-rhacs-console-dashboard.png[RHACS console]

====
Congrats! Half way there.
====

=== OpenShift admin access verification

OpenShift admin access verification involves ensuring that users have the appropriate permissions and roles assigned to them for managing the OpenShift cluster. This can be done by checking the user roles and bindings within the cluster. You'll be verifying your permissions using the oc command-line tool.

There are TWO clusters we need to verify access too.

==== Verify access to the EKS cluster

[source,sh,subs="attributes",role=execute]
----
oc config use-context eks-admin
----

*Sample output*
[source,bash]
----
[lab-user@bastion ~]$oc config use-context eks-admin
Switched to context "eks-admin".
----

[source,sh,subs="attributes",role=execute]
----
oc whoami
oc get nodes -A
----

*Sample output*
[source,bash]
----
[lab-user@bastion ~]$ oc whoami
kubectl get nodes -A
Error from server (NotFound): the server could not find the requested resource (get users.user.openshift.io ~)
NAME                                            STATUS   ROLES    AGE    VERSION
ip-<IP_ADDRESS>.us-east-2.compute.internal    Ready    <none>   163m   v1.28.8-eks-ae9a62a
ip-<IP_ADDRESS>.us-east-2.compute.internal   Ready    <none>   163m   v1.28.8-eks-ae9a62a
ip-<IP_ADDRESS>.us-east-2.compute.internal   Ready    <none>   163m   v1.28.8-eks-ae9a62a
----

We should not have access with the *oc* command as it is an OpenShift command but you can see the EKS nodes and their information.

==== Verify access to the OpenShift cluster

[source,sh,subs="attributes",role=execute]
----
oc config use-context admin
----

*Sample output*
[source,bash]
----
[lab-user@bastion ~]$oc config use-context eks-admin
Switched to context "admin".
----

[source,sh,subs="attributes",role=execute]
----
oc whoami
oc get nodes -A
----

*Sample output*
[source,bash]
----
[lab-user@bastion ~]$ oc whoami
oc get nodes -A
system:admin
NAME                                        STATUS   ROLES                  AGE    VERSION
<OCP_IP>0.us-east-2.compute.internal   Ready    worker                 4h1m   v1.27.11+749fe1d
<OCP_IP>.us-east-2.compute.internal    Ready    control-plane,master   4h7m   v1.27.11+749fe1d
<OCP_IP>.us-east-2.compute.internal    Ready    control-plane,master   4h7m   v1.27.11+749fe1d
<OCP_IP>.us-east-2.compute.internal     Ready    worker                 4h2m   v1.27.11+749fe1d
<OCP_IP>.us-east-2.compute.internal   Ready    control-plane,master   4h7m   v1.27.11+749fe1d
<OCP_IP>.us-east-2.compute.internal     Ready    worker                 4h2m   v1.27.11+749fe1d
----

And you will see the OCP role this time with the *oc* command since we are now working on the OpenShift cluster.

IMPORTANT: We will be working with the OpenShift cluster in all modules unless otherwise specified. 

=== roxctl CLI verification 

Next, Let's verify that we have access ot the RHACS Central Service.

[source,sh,subs="attributes",role=execute]
----
export ROX_CENTRAL_ADDRESS={acs_route}
roxctl --insecure-skip-tls-verify -e "$ROX_CENTRAL_ADDRESS:443" central whoami
----

*Sample output*
[source,bash]
----
UserID:
	auth-token:718744a9-9548-488b-a8b9-07b2c59ea5e6
User name:
	anonymous bearer token "pipelines-ci-token" with roles [Admin] (jti: 718744a9-9548-488b-a8b9-07b2c59ea5e6, expires: 2025-04-03T15:15:06Z)
Roles:
	- Admin
Access:
	rw Access
	rw Administration
	rw Alert
	rw CVE
	rw Cluster
	rw Compliance
	rw Deployment
	rw DeploymentExtension
	rw Detection
	rw Image
	rw Integration
	rw K8sRole
	rw K8sRoleBinding
	rw K8sSubject
	rw Namespace
	rw NetworkGraph
	rw NetworkPolicy
	rw Node
	rw Secret
	rw ServiceAccount
	rw VulnerabilityManagementApprovals
	rw VulnerabilityManagementRequests
	rw WatchedImage
	rw WorkflowAdministration
----

This output is showing that you have unrestricted access to the RHACS product. these permissions can be seen in the RHACS Access Control tab that we will review later.

image::01-rhacs-access-control.png[RHACS access control]

== Setup our workshop applications

=== Build a container image

In this section we will download the "Java app" application, give it a new tag and push the image to Quay. Later we will deploy the image to the OpenShift Cluster and use it in future modules.

. Let's export a few variable to make our life easier. These Variables will remain in the .bashrc file to be save incase you have to refresh the terminal.

[source,sh,subs="attributes",role=execute]
----
export QUAY_USER={quay_admin_username}
----

[start=2]

. Set the Quay URL variable 

[source,sh,subs="attributes",role=execute]
----
export QUAY_URL=$(oc -n quay-enterprise get route quay-quay -o jsonpath='{.spec.host}')
----

IMPORTANT: Verify that the variables are correct

[source,sh,subs="attributes",role=execute]
----
echo $QUAY_USER
echo $QUAY_URL
----

[start=3]
. Using the terminal on the bastion host, login to quay using the Podman CLI as shown below:

[source,sh,subs="attributes",role=execute]
----
podman login $QUAY_URL
----

NOTE: Use the quay admin credentials, Username: *{quay_admin_username}* & password: *{quay_admin_password}*. You can create unique user and group credentials in Quay for proper segmentation. 

*Sample output*
[source,bash]
----
Username: quayadmin
Password:
Login Succeeded!
----

[start=4]
. Pull the Java container image with the following CLI command:

[source,sh,subs="attributes",role=execute]
----
podman pull quay.io/jechoisec/ctf-web-to-system-01
----

*Sample output*
[source,bash]
----
Trying to pull quay.io/jechoisec/ctf-web-to-system-01:latest...
Getting image source signatures
Copying blob 37aaf24cf781 done 
...
...
Copying config 1cbb2b7908 done  
Writing manifest to image destination
1cbb2b79086961e34d06f301b2fa15d2a7e359e49cfe67c06b6227f6f0005149
----

[start=5]
. Now that you have a copy of the Java container image locally. You must tag the image before pushing it to Quay. 

[source,sh,subs="attributes",role=execute]
----
podman tag quay.io/jechoisec/ctf-web-to-system-01 $QUAY_URL/$QUAY_USER/ctf-web-to-system:1.0
----

NOTE: Quay will automatically create a private registry to store our Java appplication. We will need to make it a public repository to be able to pull the miage without credentials. We will do this in the following module

[start=6]
. The last step is to push the image to Quay.

[source,sh,subs="attributes",role=execute]
----
podman push $QUAY_URL/$QUAY_USER/ctf-web-to-system:1.0 --remove-signatures
----

*Sample output*
[source,bash]
----
Copying blob 3113fb957b33 done 
...
...
Copying config 1cbb2b7908 done  
Writing manifest to image destination
----

[start=6]

Perfect! 

== Red Hat Quay

Red Hat Quay is an enterprise-quality registry for building, securing and serving container images. It provides secure storage, distribution, governance of containers and cloud-native artifacts on any infrastructure.

To get started, make sure that you are logged in to Red Hat Quay and have access to the newly created *quayadmin/ctf-web-to-system* repository


=== Red Hat^(R)^ Quay console web console 

Next, access the Quay web console.

*Procedure*

[start=1]
. Log into the Quay console at {quay_console_url}

. Enter the Quay credentials.

[cols="1,1"]
|===
*Quay Console Username:* | {quay_admin_username} |
*Quay Console Password:* | {quay_admin_password} |
|===


image::01-quay-login.png[quay login]

image::01-quay-dashboard.png[quay console]

=== Browse the registry

So far in the setup module we downloaded built and pushed an insecure java application called *ctf-web-to-system*. Now it's time to deploy it to the OpenShift Cluster. To do this we will need to make the registry that we created public. 

Let's take a look at our application in the registry.

image::quay-login.png[link=self, window=blank, width=100%]


.Procedure
. Next, click on the *ctf-web-to-system* repository. 

image::quay-repo.png[link=self, window=blank, width=100%]

On the left hand side of the window you should see the following icons labeled in order from top to bottom,

- Information
- Tags
- Tag History
- Usage Logs
- Settings

image::quay-sidebar.png[link=self, window=blank, width=100%]

The information tab shows you information such as;

- Podman and Docker commands
- Repository activity
- The repository description. 

image::quay-information.png[link=self, window=blank, width=100%]

[start=2]
. Click on the *Tags* icon. 

image::quay-tags.png[link=self, window=blank, width=100%]

This tab displays all of the images and tags that have been upladed, providing information such as fixable vulnerabilities, the image size and allows for bulk changes to images based on the security posture. 

image::quay-tags-security.png[link=self, window=blank, width=100%]

[start=3]
. Click on the *Tags History* icon. This tab simply displays the container images history over time. 

image::quay-tags-history.png[link=self, window=blank, width=100%]

[start=4]
. Click on the *Usage Logs* icon. 

This tab displays the usage over time along with details about who/how the images were pushed to the cluster. 

image::quay-usage-logs.png[link=self, window=blank, width=100%]

You can see that you (The "quayadmin") pushed an image tagged 1.0 to the repository today. 

[start=5]
. Lastly click on the *Settings* icon. 

image::quay-settings.png[link=self, window=blank, width=100%]

In this tab you can add/remove users and update permissions, alter the privacy of the repository, and even schedule alerts based on found vulnerabilities.

[start=6]
. Make your repository public before deploying our application in the next step by clicking the *Make Public* button under `Repository Visability`

IMPORTANT: Make sure to make the repository public. Otherwise we will not be able to deploy the application in the next step.

image::quay-make-public.png[link=self, window=blank, width=100%]

[start=7]
. Click OK

image::quay-make-public-ok.png[link=self, window=blank, width=100%]

[[vulnerability-scanning-with-quay]]

=== Vulnerability Scanning with Quay

Red Hat Quay can also help with securing our environments by performing a security scan on any images added to our registry, and advise which ones are potentially fixable.

Use the following procedure to check the security scan results for our Java container image you have uploaded.

. Click on the *Tags* icon on the left side of the screen like before.

image::quay-tags.png[link=self, window=blank, width=100%]

NOTE: You may need to click the checkbox near the image you would would like more information on, but the column for *Security Scan* should populate.

[start=2]
. By default, the security scan color codes the vulnerabilities, you can hover over the security scan for more information.

image::quay-scan-hover.png[link=self, window=blank, width=100%]

NOTE: The Java container image we are using in this lab shows 12 vulnerabilities, with 1 high vulnerabilities. This number will change with time and will be different between container scanners for a variety of reasons such as reporting mechanisms, vulnerability feeds and operating system support. 

[start=3]
. Click on the list of vulnerabilities to see a more detailed view.

image::quay-security-detailed.png[link=self, window=blank, width=100%, Image Security Details] 

[start=4]
. Click on a vulnerabile package on the left menu to get more information about the vulnerability and see what you have to do to fix the issue.

image::quay-vuln-detailed.png[link=self, window=blank, width=100%]

NOTE: Toggling for fixable/unfixable vulnerabilities is an excellent way for developers to understand what is within their responsibility for fixing. For example, since we are using an older version of Java, many fixes are available for these common issues. 

Congratulations, you now know how to examine images in your registry for potential vulnerabilities before deploying into your environment.

[subs=attributes]


== Deploy the workshop applications

IMPORTANT: You will need to ensure that the variables are set before running the following commands. 

[source,sh,subs="attributes",role=execute]
----
echo $QUAY_USER
echo $QUAY_URL
----

Our insecure demo applications come from a variety of public GitHub repositories and sources. Including the Java application that you just pushed to Quay. Let's deploy them into our cluster.

[start=5]
. Run the following commands in the terminal

|====
This command downloads a bunch of Kubernetes manifests to deploy to OpenShift. We also add the location of the local repository for our ctf-web-to-system application. 
|====

[source,sh,subs="attributes",role=execute]
----
git clone https://github.com/mfosterrox/demo-apps.git demo-apps
export TUTORIAL_HOME="$(pwd)/demo-apps"
----

|====
This command updates the ctf-w2s.yml with your local Quay repository information.
|====

[source,sh,subs="attributes",role=execute]
----
sed -i "s|CHANGEME|$QUAY_URL/$QUAY_USER/ctf-web-to-system:1.0|g" $TUTORIAL_HOME/kubernetes-manifests/ctf-web-to-system/ctf-w2s.yml
----

|====
This command applys the manifests to OpenShift
|====

[source,sh,subs="attributes",role=execute]
----
oc apply -f $TUTORIAL_HOME/kubernetes-manifests/ --recursive
oc apply -f $TUTORIAL_HOME/openshift-pipelines/ --recursive
----

[IMPORTANT]
You should see warnings such as: Warning: would violate PodSecurity "restricted:latest": unrestricted capabilities (container "Java" must set securityContext.capabilities.drop=["ALL"]) this is because we are deploying flawed container configurations and vulnerable container applications into the OpenShift cluster.

|====
And this command triggers a vulnerability scan by RHACS to updates the vulnerability results. 
|====

[source,sh,subs="attributes",role=execute]
----
roxctl --insecure-skip-tls-verify -e "$ROX_CENTRAL_ADDRESS:443" image scan --image=$QUAY_URL/$QUAY_USER/ctf-web-to-system:1.0 --force
----

[start=6]
. Wait a few seconds then run the following command to ensure that the applications are up and running

[source,bash,role="execute"]
----
kubectl get deployments -l demo=roadshow -A
----

*Output*
```bash
NAMESPACE    NAME                READY   UP-TO-DATE   AVAILABLE   AGE
backend      api-server          1/1     1            1           7m11s
default      api-server          1/1     1            1           6m38s
default      ctf-web-to-system   1/1     1            1           7m17s
default      frontend            1/1     1            1           6m32s
default      juice-shop          1/1     1            1           7m14s
default      rce                 1/1     1            1           6m35s
default      reporting           1/1     1            1           6m41s
frontend     asset-cache         1/1     1            1           7m3s
medical      reporting           1/1     1            1           6m53s
operations   jump-host           1/1     1            1           6m48s
payments     visa-processor      1/1     1            1           6m45s
```

IMPORTANT: Please ensure the deploy application are deployed to your cluster before moving onto the next module. 

[NOTE]
The main focus needs to be that the *ctf-web-to-system* application deployed properly. 

== Summary

image::https://media.giphy.com/media/v1.Y2lkPTc5MGI3NjExbnY0NDA0ZnJqNXh6cGNqeHNxZGd5Zm5qMnlpOHhrbm1hY2pwcG5ydSZlcD12MV9pbnRlcm5hbF9naWZfYnlfaWQmY3Q9Zw/p18ohAgD3H60LSoI1C/giphy.gif[link=self, window=blank, width=100%, class="center"]

Beautiful!

In this module, you got access to all of the lab UI's and interfaces including the Showroom lab enviroment (Where you are reading this sentence). You downloaded and deployed some very insecure applications and setup the lab full of examples to dive into. 

On to *Visibility and Navigation*!!