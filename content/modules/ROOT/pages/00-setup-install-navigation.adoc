= Lab setup and introduction

== Module goals

* Access all of the applications in the lab environment
* Deploy our default insecure applications
* Ensuring OpenShift pipelines are properly setup

== Accessing the workshop

Let's start by ensuring you have access to all the necessary resources to complete this lab. 

TIP: Always open the URL by opening in a new tab. For example on a Mac *<cmd> click*

=== Access the Red Hat^(R)^ OpenShift Container Platform (OCP) web console 

First, make sure you can access the Red Hat^(R)^ OCP console web console.

*Procedure*

[start=1]
. Log into the OCP console at `{web_console_url}`
. Click the *rhsso* option

IMPORTANT: If the variables on the screen are unavailable, please inform your workshop administrator.

image::01-ocp-login-admin.png[OpenShift console]

[start=3]
. Enter the OCP credentials 

[cols="1,1"]
|===
*User:*| {openshift_admin_user} |
*Password:*| {openshift_admin_password} |
|===


[start=4]
. Enter the OpenShift username *{openshift_admin_user}* and password: *{openshift_admin_password}*

image::01-ocp-login-password.png[OpenShift console login]

=== Access the Red Hat^(R)^ Advanced Cluster Security (RHACS) web console 

Red Hat Advanced Cluster Security for Kubernetes is a Kubernetes-native security platform that equips you to build, deploy, and run cloud-native applications with more security. The solution helps protect containerized Kubernetes workloads in all major clouds and hybrid platforms, including Red Hat OpenShift, Amazon Elastic Kubernetes Service (EKS), Microsoft Azure Kubernetes Service (AKS), and Google Kubernetes Engine (GKE).

---

Let's ensure that you have access to the RHACS web console.

*Procedure*

[start=1]
. Log into the RHACS console at `{acs_route}`
. Click the "Advanced" button in your browser

image::01-rhacs-advanced.png[RHACS login not private] 

[start=3]
. Click "Proceed to {acs_route}"

image::01-rhacs-proceed.png[RHACS login proceed]

[start=4]
. Enter the RHACS credentials 

[cols="1,1"]
|===
*RHACS Console Username:* | {acs_portal_username} |
*RHACS Console Password:* | {acs_portal_password} |
|===

image::01-rhacs-login.png[RHACS console]

image::01-rhacs-console-dashboard.png[RHACS console]

=== OpenShift admin access verification

OpenShift admin access verification involves ensuring that users have the appropriate permissions and roles assigned to them for managing the OpenShift cluster. This can be done by checking the user roles and bindings within the cluster. You'll be verifying your permissions using the oc command-line tool.

There are *TWO clusters* we need to verify access too.

*Verify access to the EKS cluster*

[source,sh,subs="attributes",role=execute]
----
oc config use-context eks-admin
----

[.console-output]
[source,bash,subs="+macros,+attributes"]
----

[lab-user@bastion ~]$oc config use-context eks-admin
Switched to context "eks-admin".
----

*Verify access to the OpenShift cluster*

Next, let's switch to the OpenShift cluster running and do our work (for now) in the OpenShift cluster

*Procedure*

[start=1]
. Run the following command

[source,sh,subs="attributes",role=execute]
----
oc config use-context admin
----

[.console-output]
[source,bash,subs="+macros,+attributes"]
----
[lab-user@bastion ~]$oc config use-context eks-admin
Switched to context "admin".
----

[start=2]
. Verify access to the OpenShift cluster and the available nodes

[source,sh,subs="attributes",role=execute]
----
oc whoami
oc get nodes -A
----

[.console-output]
[source,bash,subs="+macros,+attributes"]
----
[lab-user@bastion ~]$ oc whoami
oc get nodes -A
system:admin
NAME                                        STATUS   ROLES                  AGE    VERSION
<OCP_IP>0.us-east-2.compute.internal   Ready    worker                 4h1m   v1.27.11+749fe1d
<OCP_IP>.us-east-2.compute.internal    Ready    control-plane,master   4h7m   v1.27.11+749fe1d
<OCP_IP>.us-east-2.compute.internal    Ready    control-plane,master   4h7m   v1.27.11+749fe1d
<OCP_IP>.us-east-2.compute.internal    Ready    worker                 4h2m   v1.27.11+749fe1d
<OCP_IP>.us-east-2.compute.internal    Ready    control-plane,master   4h7m   v1.27.11+749fe1d
<OCP_IP>.us-east-2.compute.internal    Ready    worker                 4h2m   v1.27.11+749fe1d
----

You will now see the OCP role using the *oc* command, as we are currently working on the OpenShift cluster

NOTE: We will be working with the OpenShift cluster in all modules unless otherwise specified. 

=== roxctl CLI verification 

Next, verify that we have access to the RHACS Central Service.

*Procedure*

[start=1]
. Run the following command.

====
This command uses variables saved in the ~/.bashrc file to authenticate with the RHACS Central Service.
====

[source,sh,subs="attributes",role=execute]
----
roxctl --insecure-skip-tls-verify -e "$ROX_CENTRAL_ADDRESS:443" central whoami
----

[.console-output]
[source,bash,subs="+macros,+attributes"]
----
UserID:
	auth-token:718744a9-9548-488b-a8b9-07b2c59ea5e6
User name:
	anonymous bearer token "pipelines-ci-token" with roles [Admin] (jti: 718744a9-9548-488b-a8b9-07b2c59ea5e6, expires: 2025-04-03T15:15:06Z)
Roles:
	- Admin
Access:
	rw Access
	rw Administration
	rw Alert
	rw CVE
	rw Cluster
	rw Compliance
	rw Deployment
	rw DeploymentExtension
	rw Detection
	rw Image
	rw Integration
	rw K8sRole
	rw K8sRoleBinding
	rw K8sSubject
	rw Namespace
	rw NetworkGraph
	rw NetworkPolicy
	rw Node
	rw Secret
	rw ServiceAccount
	rw VulnerabilityManagementApprovals
	rw VulnerabilityManagementRequests
	rw WatchedImage
	rw WorkflowAdministration
----

NOTE: This output is showing that you have unrestricted access to the RHACS product. These permissions can be seen in the **RHACS Access Control** tab that we will review later.

image::01-rhacs-access-control.png[RHACS access control]


== Setup our workshop applications


Great job!

You now have access to the core apps. Next, you'll deploy insecure apps into the OpenShift cluster, including Quay. These apps will be the main use cases we look at during the workshop.
[[build-a-container-image]]
=== Build a container image

In this section, we will download the "*Java app*" give it a new tag, and push the image to Quay. Later, we'll deploy the image to the OpenShift Cluster and use it in future modules.

Let's export a few variables to make things easier. These variables will stay in the .bashrc file so they're saved in case you need to refresh the terminal.

TIP: With the variables saved in the ~/.bashrc file you will not have to declare them again in the future. 

*Procedure*

[start=1]
. Run the following command.

[source,sh,subs="attributes",role=execute]
----
echo export QUAY_USER={quay_admin_username} >> ~/.bashrc
QUAY_USER={quay_admin_username}
----

[start=2]

. Set the Quay URL variable 

[source,sh,subs="attributes",role=execute]
----
echo export QUAY_URL=$(oc -n quay-enterprise get route quay-quay -o jsonpath='{.spec.host}') >> ~/.bashrc
QUAY_URL=$(oc -n quay-enterprise get route quay-quay -o jsonpath='{.spec.host}')
----

IMPORTANT: Verify that the variables are correct

[source,sh,subs="attributes",role=execute]
----
echo $QUAY_USER
echo $QUAY_URL
----

[.console-output]
[source,bash,subs="+macros,+attributes"]
----
[lab-user@bastion ~]$ echo $QUAY_USER
echo $QUAY_URL
quayadmin
quay-bq65l.apps.cluster-bq65l.bq65l.sandbox209.opentlc.com
----

[start=3]
. Using the terminal on the bastion host, login to quay using the Podman CLI as shown below:

[source,sh,subs="attributes",role=execute]
----
podman login $QUAY_URL
----

NOTE: Use the quay admin credentials to sign in. 

[cols="2,2"]
|===
*Quay Console Username:* | *{quay_admin_username}* |
*Quay Console Password:* | *{quay_admin_password}* |
|===

[.console-output]
[source,bash,subs="+macros,+attributes"]
----
Username: quayadmin
Password:
Login Succeeded!
----

[start=4]
. Pull the Java container image with the following CLI command:

[source,sh,subs="attributes",role=execute]
----
podman pull quay.io/jechoisec/ctf-web-to-system-01
----

[.console-output]
[source,bash,subs="+macros,+attributes"]
----
Trying to pull quay.io/jechoisec/ctf-web-to-system-01:latest...
Getting image source signatures
Copying blob 37aaf24cf781 done 
...
...
Copying config 1cbb2b7908 done  
Writing manifest to image destination
1cbb2b79086961e34d06f301b2fa15d2a7e359e49cfe67c06b6227f6f0005149
----

[start=5]
. Now that you have a copy of the Java container image locally. You must tag the image before pushing it to Quay. 

[source,sh,subs="attributes",role=execute]
----
podman tag quay.io/jechoisec/ctf-web-to-system-01 $QUAY_URL/$QUAY_USER/ctf-web-to-system:1.0
----

NOTE: Quay will automatically create a private registry to store our Java application since we have admin access. To deploy the app, you'll need to make the repository public so you can pull the image without credentials.

[start=6]
. The last step is to push the image to Quay.

[source,sh,subs="attributes",role=execute]
----
podman push $QUAY_URL/$QUAY_USER/ctf-web-to-system:1.0 --remove-signatures
----

[.console-output]
[source,bash,subs="+macros,+attributes"]
----
Copying blob 3113fb957b33 done 
...
...
Copying config 1cbb2b7908 done  
Writing manifest to image destination
----

[start=6]

Perfect! 

== Red Hat Quay

Red Hat Quay is an enterprise-quality registry for building, securing and serving container images. It provides secure storage, distribution, governance of containers and cloud-native artifacts on any infrastructure.

To get started, make sure that you are logged in to Red Hat Quay and have access to the newly created *quayadmin/ctf-web-to-system* repository

=== Red Hat^(R)^ Quay web console 

In the next steps, you'll go through a basic overview of Quay, review the Java app, and make the container image repository public. This way, you can successfully deploy the container image into the OpenShift cluster.

*Procedure*

[start=1]
. Log into the Quay console at {quay_console_url}

. Enter the Quay credentials.

[cols="2,2"]
|===
*Quay Console Username:* | {quay_admin_username} |
*Quay Console Password:* | {quay_admin_password} |
|===

image::01-quay-login.png[quay login]

image::01-quay-dashboard.png[quay console]

=== Browse the registry

So far in the setup module we downloaded built and pushed an insecure java application called *ctf-web-to-system*. Now it's time to deploy it to the OpenShift Cluster. To do this we will need to make the registry that we created public. 

*Procedure*

Let's take a look at our application in the registry.

[start=1]
. First, ensure that the *ctf-web-to-system* repository is available. If there are no repositories available please redo the *Build a container image* section to ensure that the image was correctly pushed to the repository. 

image::quay-login.png[link=self, window=blank, width=100%]

[start=2]

. Next click on the *ctf-web-to-system* repository. 

image::quay-repo.png[link=self, window=blank, width=100%]

On the left hand side of the window you should see the following icons labeled in order from top to bottom,

- Information
- Tags
- Tag History
- Usage Logs
- Settings

image::quay-sidebar.png[link=self, window=blank, width=100%]

The information tab shows you information such as;

- Podman and Docker commands
- Repository activity
- The repository description. 

image::quay-information.png[link=self, window=blank, width=100%]

[start=2]
. Click on the *Tags* icon. 

image::quay-tags.png[link=self, window=blank, width=100%]

This tab displays all of the images and tags that have been upladed, providing information such as fixable vulnerabilities, the image size and allows for bulk changes to images based on the security posture. 

image::quay-tags-security.png[link=self, window=blank, width=100%]

[start=3]
. Click on the *Tags History* icon. This tab simply displays the container images history over time. 

image::quay-tags-history.png[link=self, window=blank, width=100%]

[start=4]
. Click on the *Usage Logs* icon. 

This tab displays the usage over time along with details about who/how the images were pushed to the cluster. 

image::quay-usage-logs.png[link=self, window=blank, width=100%]

NOTE: You can see that you, the "quayadmin", pushed an image tagged 1.0 to the repository today. 
 
[start=5]
. Lastly click on the *Settings* icon. 

In this tab you can add/remove users and update permissions, alter the privacy of the repository, and even schedule alerts based on found vulnerabilities.

image::quay-settings.png[link=self, window=blank, width=100%]

[start=6]
. Make your repository public before deploying our application in the next step by clicking the *Make Public* button under `Repository Visibility`

image::quay-make-public.png[link=self, window=blank, width=100%]

IMPORTANT: Make sure to make the repository public. Otherwise we will not be able to deploy the application in the next step.

[start=7]
. Click OK

image::quay-make-public-ok.png[link=self, window=blank, width=100%]

====
You should be able to see the repository icon lose its red lock symbol now that it is public. This will allow you to deploy the image directly from the local Quay repository.
====

[[vulnerability-scanning-with-quay]]

=== Vulnerability Scanning with Quay

Red Hat Quay can also help with securing our environments by performing a security scan on any images added to our registry, and advise which ones are potentially fixable.

Use the following procedure to check the security scan results for our Java container image you have uploaded.

*Procedure*

. Click on the *Tags* icon on the left side of the screen like before.

NOTE: You may need to click the checkbox near the image you would would like more information on, but the column for *Security Scan* should populate.

image::quay-tags.png[link=self, window=blank, width=100%]

[start=2]
. By default, the security scan color codes the vulnerabilities, you can hover over the security scan for more information.

NOTE: The Java container image we are using in this lab shows 21 vulnerabilities, with 5! critical vulnerabilities. This number will change with time and will be different between container scanners for a variety of reasons such as reporting mechanisms, vulnerability feeds and operating system support. 

image::quay-scan-hover.png[link=self, window=blank, width=100%]

[start=3]
. Click on the list of vulnerabilities to see a more detailed view.

image::quay-security-detailed.png[link=self, window=blank, width=100%, Image Security Details] 

[start=4]
. Click on a vulnerabile package on the left menu to get more information about the vulnerability and see what you have to do to fix the issue.

image::quay-vuln-detailed.png[link=self, window=blank, width=100%]

TIP: Toggling for fixable/unfixable vulnerabilities is an excellent way for developers to understand what is within their responsibility for fixing. For example, since we are using an older version of Java, many fixes are available for these common issues. 

Congratulations, you now know how to examine images in your registry for potential vulnerabilities before deploying into your environment.

== Deploy the workshop applications

In the final part of this module, you'll deploy several insecure applications to the OpenShift cluster. You'll scan a few of these containers using the *roxctl* CLI to understand what you're deploying and what to expect when you dive into RHACS.

IMPORTANT: Make sure the variables are set before running the following commands. If not, go back to the Quay section to redo the previous commands.

====
Run the following in the terminal and ensure you get the corrrect outputs.
====

[source,sh,subs="attributes",role=execute]
----
echo $QUAY_USER
echo $QUAY_URL
----

Our insecure demo applications come from a variety of public GitHub repositories and sources. Including the Java application that you just pushed to Quay. Let's deploy them into our cluster.

*Procedure*

[start=1]
. Run the following commands in the terminal, one after the other.

====
This command downloads a repository container dockerfiles, attack scripts, and Kubernetes manifests that you will use to deploy the containerized applications to OpenShift.  
====

[source,sh,subs="attributes",role=execute]
----
git clone https://github.com/mfosterrox/demo-apps.git demo-apps
----

====
This command sets the variable TUTORIAL_HOME to equal the working directory, allowing you to make references to various files easily.
====

[source,sh,subs="attributes",role=execute]
----
echo export TUTORIAL_HOME="$(pwd)/demo-apps" >> ~/.bashrc
export TUTORIAL_HOME="$(pwd)/demo-apps"
----

====
This command updates the ctf-w2s.yml file with your local Quay repository information. The updated Quay URL means OpenShift will pull from your local Quay instance instead of the previous Quay.io location.
====

[source,sh,subs="attributes",role=execute]
----
sed -i 's|quay\.io/jechoisec/ctf-web-to-system-01|'"$QUAY_URL/$QUAY_USER/ctf-web-to-system:1.0"'|g' $TUTORIAL_HOME/kubernetes-manifests/ctf-web-to-system/ctf-w2s.yml
----

====
This command applies the manifests to OpenShift. It includes both OpenShift Pipelines manifests and typical application manifests with a skupper example.
====

[source,sh,subs="attributes",role=execute]
----
oc apply -f $TUTORIAL_HOME/kubernetes-manifests/ --recursive
oc apply -f $TUTORIAL_HOME/skupper-demo/ --recursive
oc apply -f $TUTORIAL_HOME/openshift-pipelines/ --recursive
----

NOTE: You should see warnings such as: *Warning: would violate PodSecurity "restricted:latest": unrestricted capabilities (container "Java" must set securityContext.capabilities.drop=["ALL"])*. This is because we are deploying flawed container configurations and vulnerable container applications into the OpenShift cluster.

====
The following command triggers a vulnerability scan by RHACS, updates the vulnerability results, and filters them into a table showing only the critical vulnerabilities.
====

[source,sh,subs="attributes",role=execute]
----
roxctl --insecure-skip-tls-verify -e "$ROX_CENTRAL_ADDRESS:443" image scan --image=$QUAY_URL/$QUAY_USER/ctf-web-to-system:1.0 --force -o table --severity=CRITICAL
----

TIP: The following output can be configured using flags. You can configure different outputs (table, CSV, JSON, and sarif.) and filter for specific severities.

[.console-output]
[source,bash,subs="+macros,+attributes"]
----
[lab-user@bastion ~]$ roxctl --insecure-skip-tls-verify -e "$ROX_CENTRAL_ADDRESS:443" image scan --image=$QUAY_URL/$QUAY_USER/ctf-web-to-system:1.0 --force -o table --severity=CRITICAL
Scan results for image: quay-8h2gf.apps.cluster-8h2gf.sandbox182.opentlc.com/quayadmin/ctf-web-to-system:1.0
(TOTAL-COMPONENTS: 9, TOTAL-VULNERABILITIES: 17, LOW: 0, MODERATE: 0, IMPORTANT: 0, CRITICAL: 17)

+-------------+---------+------------------+----------+--------------------------------------------------------------------------------+---------------+
|  COMPONENT  | VERSION |       CVE        | SEVERITY |                                      LINK                                      | FIXED VERSION |
+-------------+---------+------------------+----------+--------------------------------------------------------------------------------+---------------+
|     ejs     |  2.7.4  |  CVE-2022-29078  | CRITICAL |                https://nvd.nist.gov/vuln/detail/CVE-2022-29078                 |     3.1.7     |
...
WARN:   A total of 17 unique vulnerabilities were found in 9 components
----

[start=2]
. For the last verification step. Run the following command to ensure that the applications are up and running.

[source,bash,role="execute"]
----
oc get deployments -l demo=roadshow -A
----

[.console-output]
[source,bash,subs="+macros,+attributes"]
----
[lab-user@bastion ~]$ oc get deployments -l demo=roadshow -A
NAMESPACE    NAME                READY   UP-TO-DATE   AVAILABLE   AGE
backend      api-server          1/1     1            1           4m54s
default      api-server          1/1     1            1           4m19s
default      ctf-web-to-system   1/1     1            1           5m
default      frontend            1/1     1            1           4m13s
default      juice-shop          1/1     1            1           4m57s
default      rce                 1/1     1            1           4m16s
default      reporting           1/1     1            1           4m22s
frontend     asset-cache         1/1     1            1           4m47s
medical      reporting           1/1     1            1           4m37s
operations   jump-host           1/1     1            1           4m30s
payments     visa-processor      1/1     1            1           4m26s
----

IMPORTANT: Please ensure the deploy application are deployed to your cluster before moving onto the next module especially the *ctf-web-to-system* application. 

== A task to complete on your own.

*Here is your mission*

image::https://media.giphy.com/media/v1.Y2lkPTc5MGI3NjExNnJoNHE2MXhocm52ZzFmeHVyY3JiN3F5cGFjYW00dGsycXF2bnNtbyZlcD12MV9pbnRlcm5hbF9naWZfYnlfaWQmY3Q9Zw/3ohs4lNKssQD2wewyQ/giphy.gif[link=self, window=blank, width=100%, class="center"]

*Should you choose to accept it*

Run roxctl against a few of your favorite container images. Try pulling from link:https://hub.docker.com/[docker hub] or link:https://quay.io/[quay.io]. Try modifying the command below to include your image of choice.

For example:

[.console-output]
[source,bash,subs="+macros,+attributes"]
----
[lab-user@bastion ~]$ MYIMAGE=docker.io/ubuntu             
[lab-user@bastion ~]$ roxctl --insecure-skip-tls-verify -e "$ROX_CENTRAL_ADDRESS:443" image scan --image=$MYIMAGE --force -o table --severity=CRITICAL
----

[.console-output]
[source,bash,subs="+macros,+attributes"]
----
Scan results for image: docker.io/ubuntu
(TOTAL-COMPONENTS: 0, TOTAL-VULNERABILITIES: 0, LOW: 0, MODERATE: 0, IMPORTANT: 0, CRITICAL: 0)

+-----------+---------+-----+----------+------+---------------+
| COMPONENT | VERSION | CVE | SEVERITY | LINK | FIXED VERSION |
+-----------+---------+-----+----------+------+---------------+
+-----------+---------+-----+----------+------+---------------+
----
Showing that the latest version of Ubuntu from Docker.io has 0 critical vulnerabilities.

*Your turn*

IMPORTANT: Make sure to set the image in the MYIMAGE variable, or put the image into the command.


[source,sh,subs="attributes",role=execute]
----
MYIMAGE=<Add the registry URL here>
----

[source,sh,subs="attributes",role=execute]
----
roxctl --insecure-skip-tls-verify -e "$ROX_CENTRAL_ADDRESS:443" image scan --image $MYIMAGE --force -o table --severity=CRITICAL
----

== Summary

image::https://media.giphy.com/media/v1.Y2lkPTc5MGI3NjExbnY0NDA0ZnJqNXh6cGNqeHNxZGd5Zm5qMnlpOHhrbm1hY2pwcG5ydSZlcD12MV9pbnRlcm5hbF9naWZfYnlfaWQmY3Q9Zw/p18ohAgD3H60LSoI1C/giphy.gif[link=self, window=blank, width=100%, class="center"]

*Beautiful!*

In this module, you got access to all of the lab UI's and interfaces including the Showroom lab enviroment (Where you are reading this sentence). You downloaded and deployed some very insecure applications and setup the lab full of examples to dive into. 

On to *Visibility and Navigation*!!